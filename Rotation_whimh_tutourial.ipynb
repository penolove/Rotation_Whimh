{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec0] load raw_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames=np.load(\"../../../whimh2.0/outfile_x.npy\")\n",
    "y_train=np.load(\"../../../whimh2.0/outfile_y.npy\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"MH : \" +str((y_train==0).sum() )\n",
    "print \"NM : \" +str((y_train==1).sum())\n",
    "print \"NH : \" +str((y_train==2).sum())\n",
    "print \"Me : \" +str((y_train==3).sum())\n",
    "print \"So : \" +str((y_train==4).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NM labels too much\n",
    "arrMH=np.where(y_train==0)[0]\n",
    "arrNM=np.where(y_train==1)[0]\n",
    "arrNH=np.where(y_train==2)[0]\n",
    "arrME=np.where(y_train==3)[0]\n",
    "arrSO=np.where(y_train==4)[0]\n",
    "max_=len(arrNM)\n",
    "indx=np.random.randint(max_, size=1000)\n",
    "newarr=np.concatenate((arrMH,arrNM[indx],arrNH,arrME,arrSO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames=frames[newarr]\n",
    "y_train=y_train[newarr]\n",
    "print frames.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec0-1] create rotate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "def getrotation_img(image,angel):\n",
    "    rows,cols,c = image.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),angel,1)\n",
    "    dst = cv2.warpAffine(image,M,(cols,rows))\n",
    "    return dst\n",
    "\n",
    "def rotation_all(X_train,angle):\n",
    "    for i in xrange(X_train.shape[0]):\n",
    "        angle+=random.randint(-22 ,22)\n",
    "        if i==0:\n",
    "            X_train_temp=np.reshape(getrotation_img(X_train[i],angle),[-1,32,32,3])\n",
    "        else:\n",
    "            temp=np.reshape(getrotation_img(X_train[i],angle),[-1,32,32,3])\n",
    "            X_train_temp=np.concatenate((X_train_temp, temp), axis=0)\n",
    "    print \"the rotation output : \"+str(X_train_temp.shape)\n",
    "    return X_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training set\n",
    "X_train=frames\n",
    "X_train_0=rotation_all(frames,0)\n",
    "X_train_0_1=rotation_all(frames,0)\n",
    "X_train_45=rotation_all(frames,45)\n",
    "X_train_45_1=rotation_all(frames,45)\n",
    "X_train_90=rotation_all(frames,90)\n",
    "X_train_90_1=rotation_all(frames,90)\n",
    "X_train_135=rotation_all(frames,135)\n",
    "X_train_135_1=rotation_all(frames,135)\n",
    "X_train_180=rotation_all(frames,180)\n",
    "X_train_180_1=rotation_all(frames,180)\n",
    "X_train_180_2=rotation_all(frames,180)\n",
    "X_train_225=rotation_all(frames,225)\n",
    "X_train_225_1=rotation_all(frames,225)\n",
    "X_train_270_1=rotation_all(frames,270)\n",
    "X_train_270=rotation_all(frames,270)\n",
    "X_train_315_1=rotation_all(frames,315)\n",
    "X_train_315=rotation_all(frames,315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames=np.concatenate((X_train,X_train_0,X_train_0_1,X_train_45,X_train_45_1,X_train_90,X_train_90_1,\\\n",
    "                       X_train_135,X_train_180,X_train_225,X_train_135_1,X_train_180_1,X_train_180_2\n",
    "                       ,X_train_225_1,X_train_270,X_train_315,X_train_270_1,X_train_315_1),axis=0)\n",
    "print frames.shape\n",
    "\n",
    "y_train=np.concatenate((y_train,y_train,y_train,y_train,y_train,y_train,\\\n",
    "                        y_train,y_train,y_train,y_train,y_train,y_train,\\\n",
    "                       y_train,y_train,y_train,y_train,y_train,y_train),axis=0)\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec0-2] save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"rotated_x\",frames)\n",
    "np.save(\"rotated_y\",y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sec1-0] train raw images with naive cnn first ~85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from layer_wrapper_stn import naive_cnn,evaluation,split_data\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2231, 32, 32, 3)\n",
      "(2231,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "frames = np.load(\"../../../whimh2.0/outfile_x.npy\")\n",
    "y_train = np.load(\"../../../whimh2.0/outfile_y.npy\").astype(int)\n",
    "# data is a dictionary with key X_train,y_train ,X_val,y_val\n",
    "# NM labels too much\n",
    "arrMH=np.where(y_train==0)[0]\n",
    "arrNM=np.where(y_train==1)[0]\n",
    "arrNH=np.where(y_train==2)[0]\n",
    "arrME=np.where(y_train==3)[0]\n",
    "arrSO=np.where(y_train==4)[0]\n",
    "max_=len(arrNM)\n",
    "indx=np.random.randint(max_, size=1000)\n",
    "newarr=np.concatenate((arrMH,arrNM[indx],arrNH,arrME,arrSO))\n",
    "frames=frames[newarr]\n",
    "y_train=y_train[newarr]\n",
    "print frames.shape\n",
    "print y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1784 images in training set\n",
      "there are 447 images in testing set\n"
     ]
    }
   ],
   "source": [
    "data = split_data(y_train,frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concate a naive cnn\n",
    "[x_tensor,y_tf,y_tf_2,y_logits,regu,keep_prob] = naive_cnn()\n",
    "[train_step,merged,accuracy,cross_entropy,regular_lambda,learning_rate] = evaluation(y_logits,y_tf_2,keep_prob,regu,decay_step=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create session and initialize all variables, and create writer\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#train_writer = tf.summary.FileWriter('/tmp/naive_cnn_face/train',sess.graph)\n",
    "#test_writer = tf.summary.FileWriter('/tmp/naive_cnn_face/test')\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.994395 testing acc :0.8434 /training Loss: 0.0409052 testing loss :0.495757\n",
      "training acc: 0.997758 testing acc :0.834452 /training Loss: 0.0461103 testing loss :0.501405\n",
      "training acc: 0.993834 testing acc :0.850112 /training Loss: 0.037789 testing loss :0.463464\n",
      "training acc: 0.994395 testing acc :0.854586 /training Loss: 0.0446981 testing loss :0.469493\n",
      "training acc: 0.992153 testing acc :0.845638 /training Loss: 0.0470385 testing loss :0.498369\n",
      "training acc: 0.990471 testing acc :0.8434 /training Loss: 0.0446172 testing loss :0.532713\n",
      "training acc: 0.992153 testing acc :0.832215 /training Loss: 0.0410633 testing loss :0.485993\n",
      "training acc: 0.992713 testing acc :0.847875 /training Loss: 0.0444524 testing loss :0.492963\n",
      "training acc: 0.988789 testing acc :0.850112 /training Loss: 0.0509959 testing loss :0.491449\n",
      "training acc: 0.992713 testing acc :0.852349 /training Loss: 0.0485317 testing loss :0.487769\n",
      "training acc: 0.994395 testing acc :0.847875 /training Loss: 0.0445864 testing loss :0.480243\n",
      "training acc: 0.996076 testing acc :0.836689 /training Loss: 0.0375112 testing loss :0.489861\n",
      "training acc: 0.995516 testing acc :0.856823 /training Loss: 0.0384891 testing loss :0.489934\n",
      "training acc: 0.997197 testing acc :0.852349 /training Loss: 0.0371458 testing loss :0.479808\n",
      "training acc: 0.996637 testing acc :0.852349 /training Loss: 0.0357484 testing loss :0.480736\n",
      "training acc: 0.997758 testing acc :0.865772 /training Loss: 0.0380792 testing loss :0.46859\n",
      "training acc: 0.997758 testing acc :0.847875 /training Loss: 0.0384554 testing loss :0.487333\n",
      "training acc: 0.997758 testing acc :0.845638 /training Loss: 0.0409329 testing loss :0.477851\n",
      "training acc: 0.996076 testing acc :0.856823 /training Loss: 0.0314127 testing loss :0.550823\n",
      "training acc: 0.998318 testing acc :0.850112 /training Loss: 0.0360223 testing loss :0.476907\n",
      "training acc: 0.994395 testing acc :0.852349 /training Loss: 0.034343 testing loss :0.543769\n",
      "training acc: 0.997758 testing acc :0.8434 /training Loss: 0.0327831 testing loss :0.493392\n",
      "training acc: 0.997197 testing acc :0.841163 /training Loss: 0.0465597 testing loss :0.501422\n",
      "training acc: 0.995516 testing acc :0.85906 /training Loss: 0.0329551 testing loss :0.516384\n",
      "training acc: 0.997758 testing acc :0.856823 /training Loss: 0.0375515 testing loss :0.486092\n",
      "training acc: 0.999439 testing acc :0.832215 /training Loss: 0.0406569 testing loss :0.516602\n",
      "training acc: 0.996637 testing acc :0.845638 /training Loss: 0.0356793 testing loss :0.522108\n",
      "training acc: 0.997758 testing acc :0.852349 /training Loss: 0.0394974 testing loss :0.455958\n",
      "training acc: 0.997197 testing acc :0.850112 /training Loss: 0.0356681 testing loss :0.530385\n",
      "training acc: 0.996637 testing acc :0.863535 /training Loss: 0.0350675 testing loss :0.501295\n",
      "training acc: 0.997758 testing acc :0.852349 /training Loss: 0.0334105 testing loss :0.48074\n",
      "training acc: 0.997758 testing acc :0.863535 /training Loss: 0.038955 testing loss :0.475438\n",
      "training acc: 0.998318 testing acc :0.865772 /training Loss: 0.0342072 testing loss :0.520607\n",
      "training acc: 0.997758 testing acc :0.863535 /training Loss: 0.031106 testing loss :0.514469\n",
      "training acc: 0.997197 testing acc :0.861298 /training Loss: 0.0364657 testing loss :0.503234\n",
      "training acc: 0.998318 testing acc :0.85906 /training Loss: 0.0370399 testing loss :0.495335\n",
      "training acc: 0.998318 testing acc :0.850112 /training Loss: 0.0321069 testing loss :0.519648\n",
      "training acc: 0.998318 testing acc :0.854586 /training Loss: 0.0339974 testing loss :0.486771\n",
      "training acc: 0.998879 testing acc :0.850112 /training Loss: 0.0325908 testing loss :0.513333\n",
      "training acc: 0.997197 testing acc :0.85906 /training Loss: 0.0319298 testing loss :0.502906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d41db188ea65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         sess.run(train_step, feed_dict={\n\u001b[0;32m---> 14\u001b[0;31m             x_tensor: batch_xs, y_tf: batch_ys, learning_rate:1e-4, keep_prob:0.3, regular_lambda:regular_lambda_})\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m99\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0mepoch_i\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% We'll now train in minibatches and report accuracy, loss:\n",
    "iter_per_epoch = 100\n",
    "n_epochs = 1000\n",
    "train_size = len(data['X_train'])   \n",
    "regular_lambda_=1e-1\n",
    "indices = np.linspace(0, train_size-1, iter_per_epoch)\n",
    "indices = indices.astype('int')\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    for iter_i in range(iter_per_epoch - 1):\n",
    "        batch_xs = data['X_train'][indices[iter_i]:indices[iter_i+1]]\n",
    "        batch_ys = data['y_train'][indices[iter_i]:indices[iter_i+1]]\n",
    "        sess.run(train_step, feed_dict={\n",
    "            x_tensor: batch_xs, y_tf: batch_ys, learning_rate:1e-4, keep_prob:0.3, regular_lambda:regular_lambda_})\n",
    "    \n",
    "        if iter_i % 99 ==0and epoch_i%20 == 0:\n",
    "            summary,loss,accuracy_ = sess.run([merged,cross_entropy,accuracy],\n",
    "                            feed_dict={\n",
    "                                x_tensor: data['X_train'],\n",
    "                                y_tf: data['y_train'],\n",
    "                                regular_lambda: regular_lambda_,\n",
    "                                keep_prob:1\n",
    "                            })\n",
    "\n",
    "            #train_writer.add_summary(summary, count)\n",
    " \n",
    "            summary,loss_val,accuracy_val = sess.run([merged,cross_entropy,accuracy],\n",
    "                            feed_dict={\n",
    "                                x_tensor: data['X_val'],\n",
    "                                y_tf: data['y_val'],\n",
    "                                regular_lambda: regular_lambda_,\n",
    "                                keep_prob:1\n",
    "                            })\n",
    "            #test_writer.add_summary(summary, count)\n",
    "\n",
    "            print('training acc: ' + str(accuracy_) +' testing acc :' +str(accuracy_val)+ \n",
    "                  ' /training Loss: ' + str(loss) +' testing loss :' +str(loss_val))\n",
    "            count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sec[1-1] train raw images with naive cnn +stn ~82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from layer_wrapper_stn import ceate_stn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concate a naive cnn\n",
    "[x,h_trans,regu_stn,h_fc_loc2,keep_prob]=ceate_stn()\n",
    "[_,y_tf,y_tf_2,y_logits,regu,_] = naive_cnn(x_tensor=x,keep_prob=keep_prob)\n",
    "[train_step,merged,accuracy,cross_entropy,regular_lambda,learning_rate] = evaluation(y_logits,y_tf_2,keep_prob,regu+regu_stn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create session and initialize all variables, and create writer\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#train_writer = tf.summary.FileWriter('/tmp/naive_cnn_face/train',sess.graph)\n",
    "#test_writer = tf.summary.FileWriter('/tmp/naive_cnn_face/test')\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.981502 testing acc :0.814318 /training Loss: 0.142061 testing loss :0.525047\n",
      "training acc: 0.985426 testing acc :0.812081 /training Loss: 0.12315 testing loss :0.520214\n",
      "training acc: 0.986547 testing acc :0.823266 /training Loss: 0.122815 testing loss :0.515684\n",
      "training acc: 0.984866 testing acc :0.823266 /training Loss: 0.124201 testing loss :0.520852\n",
      "training acc: 0.987668 testing acc :0.823266 /training Loss: 0.121972 testing loss :0.516779\n",
      "training acc: 0.987668 testing acc :0.825503 /training Loss: 0.123754 testing loss :0.513525\n",
      "training acc: 0.984866 testing acc :0.814318 /training Loss: 0.122888 testing loss :0.516964\n",
      "training acc: 0.985987 testing acc :0.823266 /training Loss: 0.122701 testing loss :0.519474\n",
      "training acc: 0.987668 testing acc :0.818792 /training Loss: 0.121102 testing loss :0.520903\n",
      "training acc: 0.985987 testing acc :0.827741 /training Loss: 0.12127 testing loss :0.513526\n",
      "training acc: 0.985987 testing acc :0.821029 /training Loss: 0.121588 testing loss :0.52098\n",
      "training acc: 0.983745 testing acc :0.823266 /training Loss: 0.121419 testing loss :0.520001\n",
      "training acc: 0.987668 testing acc :0.823266 /training Loss: 0.119565 testing loss :0.518575\n",
      "training acc: 0.985987 testing acc :0.816555 /training Loss: 0.121662 testing loss :0.515783\n",
      "training acc: 0.985426 testing acc :0.823266 /training Loss: 0.119178 testing loss :0.511652\n",
      "training acc: 0.985987 testing acc :0.816555 /training Loss: 0.122204 testing loss :0.524797\n",
      "training acc: 0.985987 testing acc :0.816555 /training Loss: 0.119319 testing loss :0.515369\n",
      "training acc: 0.984866 testing acc :0.823266 /training Loss: 0.118272 testing loss :0.523566\n",
      "training acc: 0.985987 testing acc :0.823266 /training Loss: 0.118101 testing loss :0.51642\n",
      "training acc: 0.987668 testing acc :0.821029 /training Loss: 0.119554 testing loss :0.521647\n",
      "training acc: 0.985987 testing acc :0.823266 /training Loss: 0.119158 testing loss :0.522096\n",
      "training acc: 0.984866 testing acc :0.821029 /training Loss: 0.122296 testing loss :0.517761\n",
      "training acc: 0.984866 testing acc :0.816555 /training Loss: 0.117899 testing loss :0.524974\n",
      "training acc: 0.986547 testing acc :0.832215 /training Loss: 0.121757 testing loss :0.517946\n",
      "training acc: 0.987668 testing acc :0.818792 /training Loss: 0.118696 testing loss :0.516503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9bceb74e8dc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         sess.run(train_step, feed_dict={\n\u001b[0;32m---> 14\u001b[0;31m             x: batch_xs, y_tf: batch_ys, learning_rate:8e-6, keep_prob:0.3, regular_lambda:regular_lambda_})\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m99\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0mepoch_i\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stream/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% We'll now train in minibatches and report accuracy, loss:\n",
    "iter_per_epoch = 15\n",
    "n_epochs = 1000\n",
    "train_size = len(data['X_train'])   \n",
    "regular_lambda_=5e-1\n",
    "indices = np.linspace(0, train_size-1, iter_per_epoch)\n",
    "indices = indices.astype('int')\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    for iter_i in range(iter_per_epoch - 1):\n",
    "        batch_xs = data['X_train'][indices[iter_i]:indices[iter_i+1]]\n",
    "        batch_ys = data['y_train'][indices[iter_i]:indices[iter_i+1]]\n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: batch_xs, y_tf: batch_ys, learning_rate:8e-6, keep_prob:0.3, regular_lambda:regular_lambda_})\n",
    "    \n",
    "        if iter_i % 99 ==0and epoch_i%20 == 0:\n",
    "            summary,loss,accuracy_ = sess.run([merged,cross_entropy,accuracy],\n",
    "                            feed_dict={\n",
    "                                x: data['X_train'],\n",
    "                                y_tf: data['y_train'],\n",
    "                                regular_lambda: regular_lambda_,\n",
    "                                keep_prob:1\n",
    "                            })\n",
    "\n",
    "            #train_writer.add_summary(summary, count)\n",
    " \n",
    "            summary,loss_val,accuracy_val = sess.run([merged,cross_entropy,accuracy],\n",
    "                            feed_dict={\n",
    "                                x: data['X_val'],\n",
    "                                y_tf: data['y_val'],\n",
    "                                regular_lambda: regular_lambda_,\n",
    "                                keep_prob:1\n",
    "                            })\n",
    "            #test_writer.add_summary(summary, count)\n",
    "\n",
    "            print('training acc: ' + str(accuracy_) +' testing acc :' +str(accuracy_val)+ \n",
    "                  ' /training Loss: ' + str(loss) +' testing loss :' +str(loss_val))\n",
    "            count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [sec2-1] rotation data on stn + naive cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "def getrotation_img(image,angel):\n",
    "    rows,cols,c = image.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),angel,1)\n",
    "    dst = cv2.warpAffine(image,M,(cols,rows))\n",
    "    return dst\n",
    "\n",
    "def rotation_all(X_train,angle):\n",
    "    for i in xrange(X_train.shape[0]):\n",
    "        angle+=random.randint(-22 ,22)\n",
    "        if i==0:\n",
    "            X_train_temp=np.reshape(getrotation_img(X_train[i],angle),[-1,32,32,3])\n",
    "        else:\n",
    "            temp=np.reshape(getrotation_img(X_train[i],angle),[-1,32,32,3])\n",
    "            X_train_temp=np.concatenate((X_train_temp, temp), axis=0)\n",
    "    print \"the rotation output : \"+str(X_train_temp.shape)\n",
    "    return X_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2231, 32, 32, 3)\n",
      "(2231,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset\n",
    "frames = np.load(\"../../../whimh2.0/outfile_x.npy\")\n",
    "y_train = np.load(\"../../../whimh2.0/outfile_y.npy\").astype(int)\n",
    "# data is a dictionary with key X_train,y_train ,X_val,y_val\n",
    "# NM labels too much\n",
    "arrMH=np.where(y_train==0)[0]\n",
    "arrNM=np.where(y_train==1)[0]\n",
    "arrNH=np.where(y_train==2)[0]\n",
    "arrME=np.where(y_train==3)[0]\n",
    "arrSO=np.where(y_train==4)[0]\n",
    "max_=len(arrNM)\n",
    "indx=np.random.randint(max_, size=1000)\n",
    "newarr=np.concatenate((arrMH,arrNM[indx],arrNH,arrME,arrSO))\n",
    "frames=frames[newarr]\n",
    "y_train=y_train[newarr]\n",
    "print frames.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n",
      "the rotation output : (2231, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "X_train=frames\n",
    "X_train_0=rotation_all(frames,0)\n",
    "X_train_0_1=rotation_all(frames,0)\n",
    "X_train_45=rotation_all(frames,45)\n",
    "X_train_45_1=rotation_all(frames,45)\n",
    "X_train_90=rotation_all(frames,90)\n",
    "X_train_90_1=rotation_all(frames,90)\n",
    "X_train_135=rotation_all(frames,135)\n",
    "X_train_135_1=rotation_all(frames,135)\n",
    "X_train_180=rotation_all(frames,180)\n",
    "X_train_180_1=rotation_all(frames,180)\n",
    "X_train_180_2=rotation_all(frames,180)\n",
    "X_train_225=rotation_all(frames,225)\n",
    "X_train_225_1=rotation_all(frames,225)\n",
    "X_train_270_1=rotation_all(frames,270)\n",
    "X_train_270=rotation_all(frames,270)\n",
    "X_train_315_1=rotation_all(frames,315)\n",
    "X_train_315=rotation_all(frames,315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40158, 32, 32, 3)\n",
      "(40158,)\n"
     ]
    }
   ],
   "source": [
    "frames=np.concatenate((X_train,X_train_0,X_train_0_1,X_train_45,X_train_45_1,X_train_90,X_train_90_1,\\\n",
    "                       X_train_135,X_train_180,X_train_225,X_train_135_1,X_train_180_1,X_train_180_2\n",
    "                       ,X_train_225_1,X_train_270,X_train_315,X_train_270_1,X_train_315_1),axis=0)\n",
    "print frames.shape\n",
    "\n",
    "y_train=np.concatenate((y_train,y_train,y_train,y_train,y_train,y_train,\\\n",
    "                        y_train,y_train,y_train,y_train,y_train,y_train,\\\n",
    "                       y_train,y_train,y_train,y_train,y_train,y_train),axis=0)\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames=np.load(\"rotated_x.npy\")\n",
    "y_train=np.load(\"rotated_y.npy\").astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from layer_wrapper_stn import naive_cnn,evaluation,split_data,weight_variable,bias_variable,create_stn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 32126 images in training set\n",
      "there are 8032 images in testing set\n"
     ]
    }
   ],
   "source": [
    "data = split_data(y_train,frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concate a naive cnn\n",
    "[x,h_trans,regu_stn,h_fc_loc2,keep_prob] = create_stn()\n",
    "[_,y_tf,y_tf_2,y_logits,regu,_] = naive_cnn(x_tensor=x,keep_prob=keep_prob)\n",
    "[train_step,merged,accuracy,cross_entropy,regular_lambda,learning_rate] = \\\n",
    "    evaluation(y_logits,y_tf_2,keep_prob,regu+regu_stn,decay_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create session and initialize all variables, and create writer\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#train_writer = tf.summary.FileWriter('/tmp/naive_cnn_face/train',sess.graph)\n",
    "#test_writer = tf.summary.FileWriter('/tmp/stn_cnn_face/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create pools\n",
    "pools=[]\n",
    "#regu\n",
    "for i in [1e-1,1e-5,1e-7]:\n",
    "    #learing rate\n",
    "    for j in [1e-3,1e-5,1e-7]:\n",
    "        #decay step\n",
    "        for k in [325,750,1500,3000]:\n",
    "            pools.append((i,j,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= current records is : (0.1, 0.001, 325) =================\n",
      "training acc: 0.15 testing acc :0.120393 /training Loss: 1.60943 testing loss :1.60943\n",
      "training acc: 0.55 testing acc :0.534736 /training Loss: 1.15505 testing loss :1.26873\n",
      "training acc: 0.675 testing acc :0.550548 /training Loss: 1.11316 testing loss :1.21788\n",
      "training acc: 0.575 testing acc :0.54009 /training Loss: 1.08327 testing loss :1.22933\n",
      "training acc: 0.65 testing acc :0.554781 /training Loss: 1.08092 testing loss :1.1958\n",
      "training acc: 0.55 testing acc :0.542331 /training Loss: 1.1303 testing loss :1.22691\n",
      "training acc: 0.625 testing acc :0.567854 /training Loss: 1.02265 testing loss :1.15675\n",
      "training acc: 0.575 testing acc :0.548182 /training Loss: 1.07618 testing loss :1.22845\n",
      "training acc: 0.7 testing acc :0.57993 /training Loss: 1.07521 testing loss :1.17005\n",
      "training acc: 0.55 testing acc :0.550797 /training Loss: 1.11996 testing loss :1.18893\n",
      "training acc: 0.625 testing acc :0.562127 /training Loss: 1.07342 testing loss :1.16673\n",
      "training acc: 0.575 testing acc :0.574452 /training Loss: 1.08585 testing loss :1.18559\n",
      "training acc: 0.675 testing acc :0.560632 /training Loss: 1.02051 testing loss :1.16744\n",
      "training acc: 0.6 testing acc :0.572709 /training Loss: 1.07417 testing loss :1.19293\n",
      "training acc: 0.65 testing acc :0.567605 /training Loss: 1.06319 testing loss :1.14939\n",
      "training acc: 0.575 testing acc :0.580179 /training Loss: 1.10995 testing loss :1.16218\n",
      "training acc: 0.675 testing acc :0.569846 /training Loss: 0.974241 testing loss :1.12186\n",
      "training acc: 0.625 testing acc :0.580553 /training Loss: 1.11559 testing loss :1.18975\n",
      "training acc: 0.675 testing acc :0.595493 /training Loss: 1.01071 testing loss :1.12646\n",
      "training acc: 0.575 testing acc :0.580553 /training Loss: 1.05024 testing loss :1.15011\n",
      "training acc: 0.7 testing acc :0.596116 /training Loss: 1.01064 testing loss :1.1172\n",
      "training acc: 0.55 testing acc :0.530503 /training Loss: 1.23902 testing loss :1.28833\n",
      "training acc: 0.725 testing acc :0.606823 /training Loss: 0.962608 testing loss :1.10195\n",
      "training acc: 0.625 testing acc :0.611429 /training Loss: 1.02418 testing loss :1.11059\n",
      "training acc: 0.7 testing acc :0.601096 /training Loss: 0.893978 testing loss :1.08827\n",
      "training acc: 0.65 testing acc :0.604706 /training Loss: 1.05972 testing loss :1.12904\n",
      "training acc: 0.75 testing acc :0.606325 /training Loss: 0.968009 testing loss :1.09131\n",
      "training acc: 0.575 testing acc :0.604582 /training Loss: 1.07615 testing loss :1.14336\n",
      "training acc: 0.65 testing acc :0.599851 /training Loss: 1.12263 testing loss :1.17358\n",
      "training acc: 0.625 testing acc :0.60869 /training Loss: 1.04687 testing loss :1.1089\n",
      "training acc: 0.7 testing acc :0.622883 /training Loss: 1.01793 testing loss :1.10385\n",
      "training acc: 0.625 testing acc :0.60869 /training Loss: 1.06837 testing loss :1.13303\n",
      "training acc: 0.725 testing acc :0.612425 /training Loss: 0.996584 testing loss :1.08667\n",
      "training acc: 0.6 testing acc :0.606823 /training Loss: 1.02752 testing loss :1.1149\n",
      "training acc: 0.7 testing acc :0.62363 /training Loss: 0.984316 testing loss :1.08998\n",
      "training acc: 0.625 testing acc :0.619148 /training Loss: 1.00214 testing loss :1.11096\n",
      "training acc: 0.725 testing acc :0.623382 /training Loss: 0.967117 testing loss :1.06422\n",
      "training acc: 0.625 testing acc :0.624129 /training Loss: 1.02124 testing loss :1.10051\n",
      "training acc: 0.7 testing acc :0.617654 /training Loss: 0.974687 testing loss :1.08325\n",
      "training acc: 0.65 testing acc :0.599228 /training Loss: 1.0082 testing loss :1.13725\n",
      "training acc: 0.775 testing acc :0.621887 /training Loss: 0.951815 testing loss :1.06426\n",
      "training acc: 0.6 testing acc :0.616036 /training Loss: 1.02032 testing loss :1.10653\n",
      "training acc: 0.75 testing acc :0.620269 /training Loss: 0.92518 testing loss :1.08158\n",
      "training acc: 0.625 testing acc :0.622136 /training Loss: 0.995474 testing loss :1.10581\n",
      "training acc: 0.75 testing acc :0.615662 /training Loss: 0.923207 testing loss :1.06136\n",
      "training acc: 0.65 testing acc :0.625125 /training Loss: 1.0068 testing loss :1.12245\n"
     ]
    }
   ],
   "source": [
    "records=dict()\n",
    "\n",
    "for regular_lambda_,learning_rate_,decay_step_ in pools:\n",
    "    tf.reset_default_graph()\n",
    "    # concate a naive cnn\n",
    "    [x,h_trans,regu_stn,h_fc_loc2,keep_prob] = create_stn()\n",
    "    [_,y_tf,y_tf_2,y_logits,regu,_] = naive_cnn(x_tensor=x,keep_prob=keep_prob)\n",
    "    [train_step,merged,accuracy,cross_entropy,regular_lambda,learning_rate] = \\\n",
    "        evaluation(y_logits,y_tf_2,keep_prob,regu+regu_stn,decay_step=decay_step_)\n",
    "    sess = tf.Session()\n",
    "    count=0\n",
    "    \n",
    "    # %% We'll now train in minibatches and report accuracy, loss:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print \"============= current records is : \" +str((regular_lambda_,learning_rate_,decay_step_))+\" =================\"\n",
    "    \n",
    "    records[(regular_lambda_,learning_rate_,decay_step_)]=10\n",
    "    test_writer = tf.summary.FileWriter('/tmp/stn_cnn_face/test/'+str((regular_lambda_,learning_rate_,decay_step_)))\n",
    "\n",
    "    iter_per_epoch = 800\n",
    "    n_epochs = 300\n",
    "    train_size = 32126  \n",
    "    indices = np.linspace(0, train_size-1, iter_per_epoch)\n",
    "    indices = indices.astype('int')\n",
    "    for epoch_i in range(n_epochs):\n",
    "        for iter_i in range(iter_per_epoch - 1):\n",
    "            batch_xs = data['X_train'][indices[iter_i]:indices[iter_i+1]]\n",
    "            batch_ys = data['y_train'][indices[iter_i]:indices[iter_i+1]]\n",
    "\n",
    "            if iter_i % 500 == 0:\n",
    "                loss,accuracy_ = sess.run([cross_entropy,accuracy],\n",
    "                                feed_dict={\n",
    "                                    x: batch_xs,\n",
    "                                    y_tf: batch_ys,\n",
    "                                    regular_lambda: regular_lambda_,\n",
    "                                    keep_prob:1\n",
    "                                })\n",
    "\n",
    "                vali_xs = data['X_val']\n",
    "                vali_ys = data['y_val']\n",
    "                summary,loss_val,accuracy_val = sess.run([merged,cross_entropy,accuracy],\n",
    "                                feed_dict={\n",
    "                                    x: vali_xs,\n",
    "                                    y_tf: vali_ys,\n",
    "                                    regular_lambda: regular_lambda_,\n",
    "                                    keep_prob:1\n",
    "                                })\n",
    "                test_writer.add_summary(summary, count)\n",
    "                \n",
    "                if(loss_val<records[(regular_lambda_,learning_rate_,decay_step_)]):\n",
    "                    records[(regular_lambda_,learning_rate_,decay_step_)]=loss_val\n",
    "                print('training acc: ' + str(accuracy_) +' testing acc :' +str(accuracy_val)+ \n",
    "                      ' /training Loss: ' + str(loss) +' testing loss :' +str(loss_val))\n",
    "                count+=1\n",
    "\n",
    "            sess.run(train_step, feed_dict={\n",
    "                x: batch_xs, y_tf: batch_ys, learning_rate:learning_rate_, keep_prob:0.6, regular_lambda:regular_lambda_})\n",
    "    sess.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1e-05, 1e-05): 0.2827673,\n",
       " (1e-05, 0.0001): 0.2912209,\n",
       " (1e-05, 0.001): 1.4536271,\n",
       " (0.0001, 1e-05): 0.27699277,\n",
       " (0.0001, 0.0001): 0.28041306,\n",
       " (0.0001, 0.001): 0.59814215,\n",
       " (0.001, 1e-05): 0.28849912,\n",
       " (0.001, 0.0001): 0.27599043,\n",
       " (0.001, 0.001): 0.51541984}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc: 0.448422 testing acc :0.44746 /training Loss: 1.60927 testing loss :1.60927\n",
      "training acc: 0.448422 testing acc :0.44746 /training Loss: 1.45631 testing loss :1.45781\n",
      "training acc: 0.51105 testing acc :0.517928 /training Loss: 1.35352 testing loss :1.35342\n",
      "training acc: 0.514412 testing acc :0.518924 /training Loss: 1.27606 testing loss :1.27445\n",
      "training acc: 0.511362 testing acc :0.517804 /training Loss: 1.26125 testing loss :1.26478\n",
      "training acc: 0.543236 testing acc :0.547435 /training Loss: 1.17982 testing loss :1.18141\n",
      "training acc: 0.557959 testing acc :0.561629 /training Loss: 1.13299 testing loss :1.13421\n",
      "training acc: 0.562162 testing acc :0.563621 /training Loss: 1.10912 testing loss :1.11665\n",
      "training acc: 0.560512 testing acc :0.559885 /training Loss: 1.11273 testing loss :1.1217\n",
      "training acc: 0.57041 testing acc :0.569597 /training Loss: 1.07028 testing loss :1.07626\n",
      "training acc: 0.573492 testing acc :0.576818 /training Loss: 1.0526 testing loss :1.05925\n",
      "training acc: 0.595499 testing acc :0.590015 /training Loss: 1.03552 testing loss :1.04689\n",
      "training acc: 0.594472 testing acc :0.591633 /training Loss: 1.03587 testing loss :1.04887\n",
      "training acc: 0.610596 testing acc :0.608815 /training Loss: 1.00686 testing loss :1.01566\n",
      "training acc: 0.600666 testing acc :0.597859 /training Loss: 0.990511 testing loss :1.00146\n",
      "training acc: 0.630331 testing acc :0.628611 /training Loss: 0.987964 testing loss :1.00303\n",
      "training acc: 0.629615 testing acc :0.626121 /training Loss: 0.976745 testing loss :0.990844\n",
      "training acc: 0.642221 testing acc :0.632595 /training Loss: 0.955629 testing loss :0.968521\n",
      "training acc: 0.628089 testing acc :0.622759 /training Loss: 0.944775 testing loss :0.960331\n",
      "training acc: 0.65268 testing acc :0.651519 /training Loss: 0.936293 testing loss :0.954383\n",
      "training acc: 0.650283 testing acc :0.642306 /training Loss: 0.934366 testing loss :0.951622\n",
      "training acc: 0.664073 testing acc :0.65513 /training Loss: 0.913535 testing loss :0.929638\n",
      "training acc: 0.65296 testing acc :0.649029 /training Loss: 0.904628 testing loss :0.92374\n",
      "training acc: 0.672664 testing acc :0.67007 /training Loss: 0.896669 testing loss :0.917727\n",
      "training acc: 0.662236 testing acc :0.654009 /training Loss: 0.904698 testing loss :0.924903\n",
      "training acc: 0.684087 testing acc :0.675548 /training Loss: 0.877946 testing loss :0.895952\n",
      "training acc: 0.668275 testing acc :0.667455 /training Loss: 0.868119 testing loss :0.889738\n",
      "training acc: 0.694079 testing acc :0.684761 /training Loss: 0.852768 testing loss :0.875755\n",
      "training acc: 0.685644 testing acc :0.678287 /training Loss: 0.859466 testing loss :0.882931\n",
      "training acc: 0.694391 testing acc :0.681773 /training Loss: 0.843617 testing loss :0.863845\n",
      "training acc: 0.684399 testing acc :0.677291 /training Loss: 0.837633 testing loss :0.864416\n",
      "training acc: 0.709674 testing acc :0.698954 /training Loss: 0.821464 testing loss :0.84871\n",
      "training acc: 0.703947 testing acc :0.693725 /training Loss: 0.828304 testing loss :0.856149\n",
      "training acc: 0.712351 testing acc :0.700075 /training Loss: 0.806914 testing loss :0.833256\n",
      "training acc: 0.708118 testing acc :0.699452 /training Loss: 0.794297 testing loss :0.825258\n",
      "training acc: 0.723183 testing acc :0.709786 /training Loss: 0.788527 testing loss :0.818898\n",
      "training acc: 0.716989 testing acc :0.703685 /training Loss: 0.79584 testing loss :0.830209\n",
      "training acc: 0.724678 testing acc :0.707296 /training Loss: 0.774269 testing loss :0.805477\n",
      "training acc: 0.721191 testing acc :0.709288 /training Loss: 0.76301 testing loss :0.799343\n",
      "training acc: 0.738062 testing acc :0.720244 /training Loss: 0.752957 testing loss :0.787992\n",
      "training acc: 0.729782 testing acc :0.710533 /training Loss: 0.767178 testing loss :0.807929\n",
      "training acc: 0.735043 testing acc :0.714766 /training Loss: 0.746786 testing loss :0.784089\n",
      "training acc: 0.73137 testing acc :0.720618 /training Loss: 0.737964 testing loss :0.778334\n",
      "training acc: 0.748957 testing acc :0.728337 /training Loss: 0.728661 testing loss :0.767764\n",
      "training acc: 0.735666 testing acc :0.719373 /training Loss: 0.753785 testing loss :0.799945\n",
      "training acc: 0.748272 testing acc :0.729333 /training Loss: 0.711511 testing loss :0.754027\n",
      "training acc: 0.742513 testing acc :0.731325 /training Loss: 0.713654 testing loss :0.759605\n",
      "training acc: 0.75789 testing acc :0.737674 /training Loss: 0.703011 testing loss :0.74917\n",
      "training acc: 0.746156 testing acc :0.726843 /training Loss: 0.717417 testing loss :0.769971\n",
      "training acc: 0.759042 testing acc :0.736803 /training Loss: 0.686733 testing loss :0.736178\n",
      "training acc: 0.756583 testing acc :0.739666 /training Loss: 0.686729 testing loss :0.737413\n",
      "training acc: 0.771649 testing acc :0.747883 /training Loss: 0.669176 testing loss :0.717944\n",
      "training acc: 0.749953 testing acc :0.73008 /training Loss: 0.701291 testing loss :0.758206\n",
      "training acc: 0.772583 testing acc :0.748257 /training Loss: 0.66175 testing loss :0.714262\n",
      "training acc: 0.760723 testing acc :0.743277 /training Loss: 0.669649 testing loss :0.725182\n",
      "training acc: 0.776536 testing acc :0.747136 /training Loss: 0.653225 testing loss :0.70945\n",
      "training acc: 0.757019 testing acc :0.736678 /training Loss: 0.679103 testing loss :0.741157\n"
     ]
    }
   ],
   "source": [
    "# %% We'll now train in minibatches and report accuracy, loss:\n",
    "iter_per_epoch = 800\n",
    "n_epochs = 200\n",
    "train_size = len(data['X_train'])   \n",
    "regular_lambda_=1e-3\n",
    "indices = np.linspace(0, train_size-1, iter_per_epoch)\n",
    "indices = indices.astype('int')\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    for iter_i in range(iter_per_epoch - 1):\n",
    "        batch_xs = data['X_train'][indices[iter_i]:indices[iter_i+1]]\n",
    "        batch_ys = data['y_train'][indices[iter_i]:indices[iter_i+1]]\n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: batch_xs, y_tf: batch_ys, learning_rate:3e-5, keep_prob:0.4, regular_lambda:regular_lambda_})\n",
    "    \n",
    "        if iter_i % 250==0:\n",
    "            loss,accuracy_ = sess.run([cross_entropy,accuracy],\n",
    "                            feed_dict={\n",
    "                                x: data['X_train'],\n",
    "                                y_tf: data['y_train'],\n",
    "                                keep_prob:1\n",
    "                            })\n",
    "\n",
    "            #train_writer.add_summary(summary, count)\n",
    "            #summary,loss_val,accuracy_val = sess.run([merged,cross_entropy,accuracy],\n",
    "            loss_val,accuracy_val = sess.run([cross_entropy,accuracy],\n",
    "                            feed_dict={\n",
    "                                x: data['X_val'],\n",
    "                                y_tf: data['y_val'],\n",
    "                                keep_prob:1\n",
    "                            })\n",
    "            #test_writer.add_summary(summary, count)\n",
    "\n",
    "            print('training acc: ' + str(accuracy_) +' testing acc :' +str(accuracy_val)+ \n",
    "                  ' /training Loss: ' + str(loss) +' testing loss :' +str(loss_val))\n",
    "            count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [sec3-1] rotation data on stn + naive cnn, testing on un-rotation images\n",
    "fail , ~0.85 "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
